{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import RobertaTokenizer, Data2VecTextModel\n",
    "from transformers import LEDModel, LEDConfig\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer, Data2VecTextModel, T5Tokenizer, LongT5Model, LongT5EncoderModel\n",
    "import nibabel\n",
    "import numpy as np\n",
    "import mne\n",
    "import pandas as pd\n",
    "import mne_bids\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import time as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sub,ses,task):\n",
    "    bids_path = mne_bids.BIDSPath(\n",
    "    subject = sub, session = ses, task=task, datatype= \"meg\",\n",
    "    root = '.')\n",
    "    \n",
    "    raw = mne_bids.read_raw_bids(bids_path)\n",
    "    raw.load_data().filter(0.5, 30.0, n_jobs=1)\n",
    "    \n",
    "    df = raw.annotations.to_data_frame()\n",
    "    df_new = pd.DataFrame(df.description.apply(eval).to_list())\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting SQD Parameters from sub-01/ses-0/meg/sub-01_ses-0_task-0_meg.con...\n",
      "Creating Raw.info structure...\n",
      "Setting channel info structure...\n",
      "Creating Info structure...\n",
      "Ready.\n",
      "Reading events from sub-01/ses-0/meg/sub-01_ses-0_task-0_events.tsv.\n",
      "Reading channel info from sub-01/ses-0/meg/sub-01_ses-0_task-0_channels.tsv.\n",
      "The stimulus channel \"STI 014\" is present in the raw data, but not included in channels.tsv. Removing the channel.\n",
      "Reading 0 ... 395999  =      0.000 ...   395.999 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 6601 samples (6.601 sec)\n",
      "\n",
      "Extracting SQD Parameters from sub-01/ses-0/meg/sub-01_ses-0_task-1_meg.con...\n",
      "Creating Raw.info structure...\n",
      "Setting channel info structure...\n",
      "Creating Info structure...\n",
      "Ready.\n",
      "Reading events from sub-01/ses-0/meg/sub-01_ses-0_task-1_events.tsv.\n",
      "Reading channel info from sub-01/ses-0/meg/sub-01_ses-0_task-1_channels.tsv.\n",
      "The stimulus channel \"STI 014\" is present in the raw data, but not included in channels.tsv. Removing the channel.\n",
      "Reading 0 ... 715999  =      0.000 ...   715.999 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 6601 samples (6.601 sec)\n",
      "\n",
      "Extracting SQD Parameters from sub-01/ses-0/meg/sub-01_ses-0_task-2_meg.con...\n",
      "Creating Raw.info structure...\n",
      "Setting channel info structure...\n",
      "Creating Info structure...\n",
      "Ready.\n",
      "Reading events from sub-01/ses-0/meg/sub-01_ses-0_task-2_events.tsv.\n",
      "Reading channel info from sub-01/ses-0/meg/sub-01_ses-0_task-2_channels.tsv.\n",
      "The stimulus channel \"STI 014\" is present in the raw data, but not included in channels.tsv. Removing the channel.\n",
      "Reading 0 ... 1191999  =      0.000 ...  1191.999 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 6601 samples (6.601 sec)\n",
      "\n",
      "Extracting SQD Parameters from sub-01/ses-0/meg/sub-01_ses-0_task-3_meg.con...\n",
      "Creating Raw.info structure...\n",
      "Setting channel info structure...\n",
      "Creating Info structure...\n",
      "Ready.\n",
      "Reading events from sub-01/ses-0/meg/sub-01_ses-0_task-3_events.tsv.\n",
      "Reading channel info from sub-01/ses-0/meg/sub-01_ses-0_task-3_channels.tsv.\n",
      "The stimulus channel \"STI 014\" is present in the raw data, but not included in channels.tsv. Removing the channel.\n",
      "Reading 0 ... 1868999  =      0.000 ...  1868.999 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 6601 samples (6.601 sec)\n",
      "\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "hp_text = []\n",
    "for i in np.arange(4):\n",
    "    temp = []\n",
    "    dd = load_data('01','0',str(i))\n",
    "    for j in np.arange(dd.shape[0]):\n",
    "        if 'word' in dd['kind'][j]:\n",
    "            temp.append(dd['word'][j])\n",
    "    hp_text.append(temp)\n",
    "print(len(hp_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if possible\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_total_layers = 12 # total number of layers in model\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_flan_layer_representations(args, text_array, remove_chars):\n",
    "    seq_len = args.sequence_length\n",
    "    nlp_model = args.nlp_model\n",
    "    word_ind_to_extract = args.word_ind_to_extract\n",
    "\n",
    "    model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    #model.config.max_decoder_position_embeddings = 16384    # Process tokens longer than 1024\n",
    "    model.eval()\n",
    "\n",
    "    # get the token embeddings\n",
    "    token_embeddings = []\n",
    "    for word in text_array:\n",
    "        #print(word)\n",
    "        current_token_embedding = get_led_token_embeddings([word], tokenizer, model, remove_chars)\n",
    "        token_embeddings.append(np.mean(current_token_embedding.detach().numpy(), 1))\n",
    "    \n",
    "    # where to store layer-wise led embeddings of particular length\n",
    "    LED = {}\n",
    "    for layer in range(n_total_layers):\n",
    "        LED[layer] = []\n",
    "    LED[-1] = token_embeddings\n",
    "    \n",
    "    Attention = {}\n",
    "    for layer in range(n_total_layers):\n",
    "        Attention[layer] = []\n",
    "\n",
    "    # Before we've seen enough words to make up the seq_len\n",
    "    # Extract index 0 after supplying tokens 0 to 0, extract 1 after 0 to 1, 2 after 0 to 2, ... , 19 after 0 to 19\n",
    "    start_time = tm.time()\n",
    "    for truncated_seq_len in range(1, 1+seq_len):\n",
    "        word_seq = text_array[:truncated_seq_len]\n",
    "        from_start_word_ind_to_extract = -1 + truncated_seq_len\n",
    "        LED, attentions = add_avrg_token_embedding_for_specific_word(word_seq, tokenizer, model, remove_chars, \n",
    "                                                            from_start_word_ind_to_extract, LED)\n",
    "        for layer in range(n_total_layers):\n",
    "            Attention[layer].append(attentions[layer].detach().numpy()[:,:,:,-1])\n",
    "        if truncated_seq_len % 100 == 0:\n",
    "            print('Completed {} out of {}: {}'.format(truncated_seq_len, len(text_array), tm.time()-start_time))\n",
    "            start_time = tm.time()\n",
    "\n",
    "    word_seq = text_array[:seq_len]\n",
    "    if word_ind_to_extract < 0: # the index is specified from the end of the array, so invert the index\n",
    "        from_start_word_ind_to_extract = seq_len + word_ind_to_extract\n",
    "    else:\n",
    "        from_start_word_ind_to_extract = word_ind_to_extract\n",
    "        \n",
    "    # Then, use sequences of length seq_len, still adding the embedding of the last word in a sequence\n",
    "    for end_curr_seq in range(seq_len, len(text_array)):\n",
    "        word_seq = text_array[end_curr_seq-seq_len+1:end_curr_seq+1]\n",
    "        LED, attentions = add_avrg_token_embedding_for_specific_word(word_seq, tokenizer, model, remove_chars,\n",
    "                                                            from_start_word_ind_to_extract, LED)\n",
    "        for layer in range(n_total_layers):\n",
    "            Attention[layer].append(attentions[layer].detach().numpy()[:,:,:,-1])\n",
    "\n",
    "        if end_curr_seq % 100 == 0:\n",
    "            print('Completed {} out of {}: {}'.format(end_curr_seq, len(text_array), tm.time()-start_time))\n",
    "            start_time = tm.time()\n",
    "\n",
    "    print('Done extracting sequences of length {}'.format(seq_len))\n",
    "    return LED, Attention\n",
    "\n",
    "# extracts layer representations for all words in words_in_array\n",
    "# encoded_layers: list of tensors, length num layers. each tensor of dims num tokens by num dimensions in representation\n",
    "# word_ind_to_token_ind: dict that maps from index in words_in_array to index in array of tokens when words_in_array is tokenized,\n",
    "#                       with keys: index of word, and values: array of indices of corresponding tokens when word is tokenized\n",
    "@torch.inference_mode()\n",
    "def predict_led_embeddings(words_in_array, tokenizer, model, remove_chars):    \n",
    "    for word in words_in_array:\n",
    "        if word in remove_chars:\n",
    "            print('An input word is also in remove_chars. This word will be removed and may lead to misalignment. Proceed with caution.')\n",
    "            return -1\n",
    "    \n",
    "    n_seq_tokens = 0\n",
    "    seq_tokens = []\n",
    "    \n",
    "    word_ind_to_token_ind = {}             # dict that maps index of word in words_in_array to index of tokens in seq_tokens\n",
    "    \n",
    "    for i,word in enumerate(words_in_array):\n",
    "        word_ind_to_token_ind[i] = []      # initialize token indices array for current word\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "            \n",
    "        for token in word_tokens:\n",
    "            if token not in remove_chars:  # don't add any tokens that are in remove_chars\n",
    "                seq_tokens.append(token)\n",
    "                word_ind_to_token_ind[i].append(n_seq_tokens)\n",
    "                n_seq_tokens = n_seq_tokens + 1\n",
    "    \n",
    "    # convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(seq_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
    "    # Use local attention, do not use global attention\n",
    "    # attention_mask = torch.ones(tokens_tensor.shape, dtype=torch.long, device=tokens_tensor.device)\n",
    "    # global_attention_mask = torch.zeros(tokens_tensor.shape, dtype=torch.long, device=tokens_tensor.device)\n",
    "\n",
    "    outputs = model(tokens_tensor, output_hidden_states=True, output_attentions=True)\n",
    "    encoder_hidden_states = outputs['hidden_states'][1:]    # This is a tuple: (layer1, layer2, ..., layer6)\n",
    "    all_layers_hidden_states = encoder_hidden_states\n",
    "    \n",
    "    return all_layers_hidden_states, word_ind_to_token_ind, outputs['attentions']\n",
    "  \n",
    "# add the embeddings for a specific word in the sequence\n",
    "# token_inds_to_avrg: indices of tokens in embeddings output to avrg\n",
    "@torch.inference_mode()\n",
    "def add_word_led_embedding(model_dict, embeddings_to_add, token_inds_to_avrg, specific_layer=-1):\n",
    "    if specific_layer >= 0:  # only add embeddings for one specified layer\n",
    "        layer_embedding = embeddings_to_add[specific_layer]\n",
    "        full_sequence_embedding = layer_embedding.cpu().detach().numpy()\n",
    "        model_dict[specific_layer].append(np.mean(full_sequence_embedding[0,token_inds_to_avrg,:],0))\n",
    "    else:\n",
    "        for layer, layer_embedding in enumerate(embeddings_to_add):\n",
    "            full_sequence_embedding = layer_embedding.cpu().detach().numpy()\n",
    "            model_dict[layer].append(np.mean(full_sequence_embedding[0,token_inds_to_avrg,:],0)) # avrg over all tokens for specified word\n",
    "    return model_dict\n",
    "\n",
    "# predicts representations for specific word in input word sequence, and adds to existing layer-wise dictionary\n",
    "#\n",
    "# word_seq: numpy array of words in input sequence\n",
    "# tokenizer: LED tokenizer\n",
    "# model: LED model\n",
    "# remove_chars: characters that should not be included in the represention when word_seq is tokenized\n",
    "# from_start_word_ind_to_extract: the index of the word whose features to extract, INDEXED FROM START OF WORD_SEQ\n",
    "# model_dict: where to save the extracted embeddings\n",
    "@torch.inference_mode()\n",
    "def add_avrg_token_embedding_for_specific_word(word_seq,tokenizer,model,remove_chars,from_start_word_ind_to_extract,model_dict):\n",
    "    \n",
    "    word_seq = list(word_seq)\n",
    "    all_sequence_embeddings, word_ind_to_token_ind, attentions = predict_led_embeddings(word_seq, tokenizer, model, remove_chars)\n",
    "    token_inds_to_avrg = word_ind_to_token_ind[from_start_word_ind_to_extract]\n",
    "    model_dict = add_word_led_embedding(model_dict, all_sequence_embeddings,token_inds_to_avrg)\n",
    "    \n",
    "    return model_dict, attentions\n",
    "\n",
    "\n",
    "# get the LED token embeddings\n",
    "@torch.inference_mode()\n",
    "def get_led_token_embeddings(words_in_array, tokenizer, model, remove_chars):    \n",
    "    for word in words_in_array:\n",
    "        if word in remove_chars:\n",
    "            print('An input word is also in remove_chars. This word will be removed and may lead to misalignment. Proceed with caution.')\n",
    "            return -1\n",
    "    \n",
    "    n_seq_tokens = 0\n",
    "    seq_tokens = []\n",
    "    \n",
    "    word_ind_to_token_ind = {}             # dict that maps index of word in words_in_array to index of tokens in seq_tokens\n",
    "    \n",
    "    for i,word in enumerate(words_in_array):\n",
    "        word_ind_to_token_ind[i] = []      # initialize token indices array for current word\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "        #print(word)\n",
    "        for token in word_tokens:\n",
    "            if token not in remove_chars:  # don't add any tokens that are in remove_chars\n",
    "                seq_tokens.append(token)\n",
    "                word_ind_to_token_ind[i].append(n_seq_tokens)\n",
    "                n_seq_tokens = n_seq_tokens + 1\n",
    "    \n",
    "    # convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(seq_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
    "    \n",
    "    # outputs = model(tokens_tensor, output_hidden_states=True)\n",
    "    # hidden_states = outputs['encoder_hidden_states']\n",
    "    # token_embeddings = hidden_states[0].cpu()\n",
    "    \n",
    "    input_embedding_module = model.base_model.get_input_embeddings()\n",
    "    token_embeddings = input_embedding_module(tokens_tensor).cpu()\n",
    "    \n",
    "    return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "  sequence_length = 5\n",
    "  nlp_model = 'bert'\n",
    "  word_ind_to_extract = -1\n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 100 out of 668: 1.9533805847167969\n",
      "Completed 200 out of 668: 1.9539532661437988\n",
      "Completed 300 out of 668: 1.9145164489746094\n",
      "Completed 400 out of 668: 1.903895378112793\n",
      "Completed 500 out of 668: 1.9026987552642822\n",
      "Completed 600 out of 668: 1.9443657398223877\n",
      "Done extracting sequences of length 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 100 out of 1503: 1.9326796531677246\n",
      "Completed 200 out of 1503: 1.882972002029419\n",
      "Completed 300 out of 1503: 1.8795444965362549\n",
      "Completed 400 out of 1503: 1.883718490600586\n",
      "Completed 500 out of 1503: 1.9046590328216553\n",
      "Completed 600 out of 1503: 1.8928685188293457\n",
      "Completed 700 out of 1503: 1.8651108741760254\n",
      "Completed 800 out of 1503: 1.9150195121765137\n",
      "Completed 900 out of 1503: 1.8833322525024414\n",
      "Completed 1000 out of 1503: 1.9162371158599854\n",
      "Completed 1100 out of 1503: 1.9118592739105225\n",
      "Completed 1200 out of 1503: 1.920710802078247\n",
      "Completed 1300 out of 1503: 1.8843324184417725\n",
      "Completed 1400 out of 1503: 1.8784525394439697\n",
      "Completed 1500 out of 1503: 1.9393837451934814\n",
      "Done extracting sequences of length 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 100 out of 2637: 1.9236600399017334\n",
      "Completed 200 out of 2637: 1.8764166831970215\n",
      "Completed 300 out of 2637: 1.8710460662841797\n",
      "Completed 400 out of 2637: 1.9004833698272705\n",
      "Completed 500 out of 2637: 1.867189884185791\n",
      "Completed 600 out of 2637: 1.9169974327087402\n",
      "Completed 700 out of 2637: 1.8932592868804932\n",
      "Completed 800 out of 2637: 1.917586326599121\n",
      "Completed 900 out of 2637: 1.876328706741333\n",
      "Completed 1000 out of 2637: 1.8639819622039795\n",
      "Completed 1100 out of 2637: 1.9781897068023682\n",
      "Completed 1200 out of 2637: 1.9419770240783691\n",
      "Completed 1300 out of 2637: 1.8882479667663574\n",
      "Completed 1400 out of 2637: 1.8897192478179932\n",
      "Completed 1500 out of 2637: 1.8712871074676514\n",
      "Completed 1600 out of 2637: 1.8815569877624512\n",
      "Completed 1700 out of 2637: 1.8747591972351074\n",
      "Completed 1800 out of 2637: 1.8732962608337402\n",
      "Completed 1900 out of 2637: 1.859283685684204\n",
      "Completed 2000 out of 2637: 1.8689508438110352\n",
      "Completed 2100 out of 2637: 1.9014508724212646\n",
      "Completed 2200 out of 2637: 1.8831040859222412\n",
      "Completed 2300 out of 2637: 1.866847038269043\n",
      "Completed 2400 out of 2637: 1.8775553703308105\n",
      "Completed 2500 out of 2637: 1.8602473735809326\n",
      "Completed 2600 out of 2637: 1.8643362522125244\n",
      "Done extracting sequences of length 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 100 out of 3753: 1.8842496871948242\n",
      "Completed 200 out of 3753: 1.8981006145477295\n",
      "Completed 300 out of 3753: 1.8666222095489502\n",
      "Completed 400 out of 3753: 1.885955572128296\n",
      "Completed 500 out of 3753: 1.8744089603424072\n",
      "Completed 600 out of 3753: 1.886655330657959\n",
      "Completed 700 out of 3753: 1.8745534420013428\n",
      "Completed 800 out of 3753: 1.8650240898132324\n",
      "Completed 900 out of 3753: 1.8990833759307861\n",
      "Completed 1000 out of 3753: 1.8790464401245117\n",
      "Completed 1100 out of 3753: 1.901090145111084\n",
      "Completed 1200 out of 3753: 1.8752076625823975\n",
      "Completed 1300 out of 3753: 1.874053716659546\n",
      "Completed 1400 out of 3753: 1.8812565803527832\n",
      "Completed 1500 out of 3753: 1.8963005542755127\n",
      "Completed 1600 out of 3753: 1.8839001655578613\n",
      "Completed 1700 out of 3753: 1.8604156970977783\n",
      "Completed 1800 out of 3753: 1.9190547466278076\n",
      "Completed 1900 out of 3753: 1.8717823028564453\n",
      "Completed 2000 out of 3753: 1.9047951698303223\n",
      "Completed 2100 out of 3753: 1.9563326835632324\n",
      "Completed 2200 out of 3753: 1.8713479042053223\n",
      "Completed 2300 out of 3753: 1.8712711334228516\n",
      "Completed 2400 out of 3753: 1.8484597206115723\n",
      "Completed 2500 out of 3753: 1.8838005065917969\n",
      "Completed 2600 out of 3753: 1.9951646327972412\n",
      "Completed 2700 out of 3753: 1.892803430557251\n",
      "Completed 2800 out of 3753: 1.8672137260437012\n",
      "Completed 2900 out of 3753: 1.8939285278320312\n",
      "Completed 3000 out of 3753: 1.8581879138946533\n",
      "Completed 3100 out of 3753: 1.8463184833526611\n",
      "Completed 3200 out of 3753: 1.8622980117797852\n",
      "Completed 3300 out of 3753: 1.860826015472412\n",
      "Completed 3400 out of 3753: 1.855417251586914\n",
      "Completed 3500 out of 3753: 1.8389091491699219\n",
      "Completed 3600 out of 3753: 1.8717832565307617\n",
      "Completed 3700 out of 3753: 1.8582212924957275\n",
      "Done extracting sequences of length 5\n"
     ]
    }
   ],
   "source": [
    "#text_array = np.load(os.getcwd() + '/stimuli_words.npy')\n",
    "remove_chars = [\",\",\"\\\"\",\"@\"]\n",
    "embeddings = []\n",
    "att_emb = []\n",
    "for i in np.arange(4):\n",
    "    temp, attention = get_flan_layer_representations(args,hp_text[i],remove_chars)\n",
    "    embeddings.append(temp)\n",
    "    att_emb.append(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"bert-base-lw-attention-\"+str(args.sequence_length), att_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
